---
title: "Krkosek Lab meeting Skills Learning - Winter Semester, 2019"
output: html_notebook
---

#### Randomization Tests

you could do something simple like a t-test between two lakes and see there's a differnece between them 
  - but they have different communities between them
But, you can't think about different communities by just plotting body size, per se

So we want quantitative differences between the two 

# How can we test if communities vary in their assemblages? 

What are our underlying distributions?

How can we put observational differences into a quantitative framework?

Neutral theory - the idea that communities are random assemblages of groups of organisms, so you might see a lake over here has some species, and a different lake has other species, and this is probably not true, but you can test things against this neutral model to compare your assemblagess
    - allows our empirical data to create it's own distribution from which the observations can be compared
    
How does this work?
    - you collect your species pool from your two lakes (our example) - and then you want to randomize those species, so you randomly populate your lakes with your species,       but hold constant the number of species in each lakes, without replacement (so if a gobie is in both lakes, it goes in your list twice)
    - you repeat this random replacement 5000 times (or some large number) and you compare the observed communities to this random distribution
        - and you compare across some variable (like lake temperature)
        
Pseudocode:
- community assemblies have packages
  -STEP BY STEP:
  -create a list of all species keeping replicate species
  -create an empty matrix with column names corresponding to species names and row entries corresponding to sites
  -have a larger loop with the number of iterations of the process
  -have an inside loop go through each site and a random number generator pulling values (without replacement)
  -the # of species observed in each site is held fixed, the numbers correspond to species in the list

#Example

-two sites, and you have food webs in each one
  -so you have differences in the species you observe, then you have interactions that are common/unique in the two sites
-two randomly chosen lakes might be different by a lot, but what's important is we don't know whether or not their differences are unique
-so we could create a null model and then compare our observed values to this null distribution
-BUT, now we have a network, and there are layers upon which you can institute your randomization
  - our null can start right from the species assemblage, so you compare your observed foodwebs against a complete random foodweb, 
    OR you can assume the speceis sampling is done appropriately and start from the interactions

- for this lab, if you were looking at the the different salmon as sites and you randomize the species of lice on the salmon and then see whether or not what you see is      less random than you'd expect

#Omnivory Debate

-general omnivore: something that feeds on multiple trophic levels 
-we see a 'lot'of omnivory in data compared to what theory says, but what is a 'lot'?
-we can compare across ocean vs. terrestrial webs, but that's a bit weird cuz they're so different 
  -we could run a null model to see if a 'lot'is actually a lot - choice of a null model can change whether these results are significant or not 
  -you'd have a number of observed omnivory modules, and if your real-life web is in the top 2.5 or bottom 2.5%, then your focal web is over or under expressed in terms of   omnivory - this is kind of the real sort of randomization (Robert May's papers?)
  -THOUGHT: could you compare a bunch of similar foodwebs to see if something like omnivory is actually over expressed? 

#Example

-you could take a food web and break it into three species motifs -- you can classify a species' role by using the motifs, and you can create a null model in this case, and instead of recreating the foodweb, you maintain the site id and the species assembly, but the identity of the fish that gets assigned to the different roles 

- choosing a null model requires thought about what you want to test and what you want to control for -- 


## Other potential models for null models:
-Niche overlap --> resource stoichiometry etc. 
-Rarefaction --> your sampling effort to whatever you're measuring (gut content analysis etc.), classic null model 

# Null models in Ecology (Nicholas J. Gotelli & Graves)

    
#### Likelihood & Multi-level Models

Example of the Toad:

Toads are apparently 'handed', they tend to use one hand more than others

H0 = hand preference is equal for both
HA = both are not equal

#p-value: the probability of obtaining data if the null hypothesis is true

The hypothesis you use is a model - binomial distribution, you can calculate the probability of different outcomes - this is a conditional probability on a hypothesis based on a model

#likelihood

'whats the probability of our data, given some model?' 

the likelihood is the probability that our outcome is what it is - it's conditional on our null hypothesis 

This tells us nothing about hypothesis testing 

we could want to know what values of the parameters in the model maximize the likelihood

so you have your data, and your calcualting the probaility of it based on the model, and trying to find the combination of parameter values that give you thehighest  likelihood - this is an optimization problem

so you go through the parameter space and calculate the maximum likelihood value - and that's the max lik. parameter value - you're searching over the possible parameter values 

if you have multiple parameters, it's always considering the space of those parameters together, not each parameter in solo

Thinking about two different things:
  1. sample space - holding parameters constant
  2. parameter space - hold outcome constant
In a single parameter problem, you can use a likelihood profile method

So likelihood is the probability of our data, conditional on the model, and sometimes, we want the probability of our model, given our data - that's where bayesian analysis comes in

#### Basyesian Analysis

the probability of our data is conditional on some parameters, and we might be interested in the parameters, or the model, given the data

Bayes theorem is from conditional probability theory

The liklihood fits in here, and the prior is the probability of the model itself - the constant ensures things sum to one

prior probability of the model or the prior distribution of the parameters,

and the posterior is on the left, you're updating it based on your prior based on your likelihood

purpose of the prior allows you to get your conditional probability in the other direction

the prior could just be your opinion - or it could be results of some experiments

an 'uninformative prior' is something like a flat distribution 

a prior introduces subjectivity into your analysis in a formal way - so you can trial different priors and see how sensitive your results are based on your prior

the bias of the prior is why bayesian inference is somewhat controversial

Look up 'Data Cloning' - allows you to return likelihood from bayesian computation - you can overwhelm your prior by stacking multiples of your dataset, and at high dataclones, you can get closer to your maximum liklihood

Bayesian computation in Rjags is appealing, 


NOTES: if you have a nice clear testable hypothesis, you want a p-value. if you're lookng at multiple models, you want AIC etc. if you want estimation and prediction, you're well off in a bayesian environment, ex. a stock assessment model. work with uninformative priors if you don't want to introduce bias 

#liklihood in linear regression:

the model we're fitting is a straight line, three parameters: mean, y-intercept and error term - that is always centered on zero and has some variance

When you minimize your squared residuals, that's similar to max liklihood 

#### Hierarchical Models

the multilevel model is saying that the intercept might vary according to some deviation that represents some structure in your dataset - your i is the dataset, and the j is the grouping factor - a clustering of observations might share things based on that cluster factor

So you're partitioning your residual variation from the variation that arises due to clustering

Essentailly dealing with the nested problem here 

So you're fitting independent lines (assuming the same slope) for each clustering of points for one random variable, and if there's multiple random variables, you can fit those lines with different slopes

####### Bayesian Methods Using JAGS

P(0|x) = P(x|0)P(0)/P(x) 
posterior = likelihood*prior

prior = information about the model from previous experiments, biologically reasonable parameters etc.

Example: 
You have four observations and you want to know the density of trees in the forest - and you see a mean of 455 - prior experiments show its usually around 800, so you use the prior proability and the posterior, and you multiply the liklihood by the prior probability, and get a scaled posterior distribution 



If you change the prior, your posterior changes - an uninformative prior (can be a flat prior which means that any of the values in your range are equally likely, but there's still limits), there's no rule of what you should do, you have to think about it

You could use datacloning to overwhelm your prior 

Bayesian pros/cons:
Pros:
  -might be more intuitive
    -treats parameters as random variables
    -deals with probabilities of parameter values rather than accept/reject
  -Can be easier to implement complex models
Cons:
  -we don't know the probability of the models
  -or the prob of the data

Problems & Solutions:

1. We don't know P(0) the prior
  -but often we do, there's usually a previous study!
  -can use an uninformative prior
  -can use data cloning
  -should test results over a range of different priors
    -if you get a bunch of different answers, you need to be very sure about why you're
    choosing what you are
2. WE don't know P(x) the data
  -can be determined by integrating over all of the posible models and or parameters but it gets ugly
  -can ignore the denoninator all together if we're just interested in relative probability
  -can use MCMC methods to integrate denominator
    -repeated random sampling
    -it's memoryless process of transitions 
    -MCMC algorithms are used to sample from posterior, circumventing the need to calculate
      -uses stochastic jumps in parameter space to find distribution
      -you often have to run it many times and make sure your chains are converging, and you
      start it in different places
      -if r.hat is under 1.1 then the chains have converged
      
Look up/use JAGS manual - the guy who wrote it is good

Data Cloning (aka Prior Feedback)
so you're using the same model but there's a k:
P(0|x) = P(x|0)^kP(0)/P(x) 
  -makes k clones of your data, then uses MCMC to generate random draws form a posterior
  based on some prior and the lconed data
  -numerical trick to giver MLE instead of Bayesian posteriors
  -allows for parameter estimability
  -overwhelms the importnace of your priors
  -does not make up for the lack of data
  the scaled variance in your estimates will decrease asymptotically as you increase the
  number of clones
    -first step is convergence, chains need to be on top of one another (i.e. r.hat <1.1)
    -now you can look at identifiability - if the dots are on the line, they're identifiable
    so its estimatable -- if it's decreasing over time, it's estimable, but  there's no hard
    fast rule
    

Multivariate Stats

#use core(df) or image(core(df)) and that'll produce a heatmap



  
  
  
  
  
  
  
  
  
  
  
  